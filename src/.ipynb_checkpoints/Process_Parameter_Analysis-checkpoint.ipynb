{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1de746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caeedb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../../In-situ Meas Data/Build Command Data/XYPT Commands/' # Directory of XYPT files\n",
    "# DATA_DIR = '../../../RTC_Images/'\n",
    "\n",
    "dt = 1E-5 # 10 microseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe030b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(DATA_DIR + 'T500_3D_Scan_Strategies_fused_layer0002.csv', header=None, names=['X', 'Y', 'P', 'T', 'deprecated'])\n",
    "data = pd.read_csv(DATA_DIR + 'RTC_XYL_DAQ_IN625_Ar_P01I2000D10_RefA400_Base.csv', header=None, names=['X', 'Y', 'P', 'T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_indx = np.where(np.array(data['T']))[0]\n",
    "X = np.array(data['X'])\n",
    "Y = np.array(data['Y'])\n",
    "P = np.array(data['P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed\n",
    "x_vel = np.zeros(X.shape)\n",
    "x_vel[1:-1] = (X[2:]-X[:-2]) / 2 / dt\n",
    "\n",
    "y_vel = np.zeros(X.shape)\n",
    "y_vel[1:-1] = (Y[2:]-Y[:-2]) / 2 / dt\n",
    "\n",
    "speed = np.sqrt(np.square(x_vel) + np.square(y_vel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58608bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direction\n",
    "vel_dir_x = x_vel / (speed + 1e-8)\n",
    "vel_dir_y = y_vel / (speed + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceleration\n",
    "x_accel = np.zeros(X.shape)\n",
    "x_accel[1:-1] = (x_vel[2:]-x_vel[:-2]) / 2 / dt\n",
    "\n",
    "y_accel = np.zeros(Y.shape)\n",
    "y_accel[1:-1] = (y_vel[2:]-y_vel[:-2]) / 2 / dt\n",
    "\n",
    "accel = np.sqrt(np.square(x_accel) + np.square(y_accel))\n",
    "accel[np.where(speed[1:] < speed[:-1])[0]+1] = accel[np.where(speed[1:] < speed[:-1])[0]+1]*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy Density\n",
    "en_den = P / (speed + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heating/Cooling\n",
    "heating = np.zeros(X.shape)\n",
    "heating[np.where(P[1:]>P[:-1]+1)[0]+1] = 1\n",
    "heating[np.where(P[1:]<P[:-1]-1)[0]+1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning\n",
    "turn_threshold = 0.01 # Tune me\n",
    "\n",
    "turning = np.zeros(X.shape)\n",
    "turn1 = np.zeros(X.shape)\n",
    "turn2 = np.zeros(X.shape)\n",
    "\n",
    "turn1[1:] = np.abs(vel_dir_x[1:]-vel_dir_x[:-1])\n",
    "turn2[1:] = np.abs(vel_dir_y[1:]-vel_dir_y[:-1])\n",
    "turning = np.any([(turn1 > turn_threshold), (turn2 > turn_threshold)], axis=0)\n",
    "turning = turning.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Heat\n",
    "R = 10 # parts are 10 mm x 10 mm\n",
    "T = 5\n",
    "max_iters = round(T/dt)\n",
    "\n",
    "residual_heat = np.zeros(X.shape[0])\n",
    "for i in tqdm(trigger_indx):\n",
    "    d_ij = np.sqrt(np.square(X[i]-X[max(0, i-max_iters):i]) + np.square(Y[i]-Y[max(0, i-max_iters):i]))\n",
    "    t_ij = np.linspace((min(max_iters, i))*dt, dt, min(max_iters, i))\n",
    "    p_j = P[max(0, i-max_iters):i]\n",
    "    residual_heat[i] += np.sum(np.square((R-d_ij)/R) * ((T-t_ij)/T) * p_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together, normalize, and save to an excel file\n",
    "P_trig = P[trigger_indx]\n",
    "speed_trig = speed[trigger_indx]\n",
    "x_dir_trig = vel_dir_x[trigger_indx]\n",
    "y_dir_trig = vel_dir_y[trigger_indx]\n",
    "accel_trig = accel[trigger_indx]\n",
    "en_den_trig = en_den[trigger_indx]\n",
    "heating_trig = heating[trigger_indx]\n",
    "turning_trig = turning[trigger_indx]\n",
    "residual_heat_trig = residual_heat[trigger_indx]\n",
    "\n",
    "pp = pd.DataFrame({'P': P_trig, 'speed': speed_trig, 'x_dir': x_dir_trig, 'y_dir': y_dir_trig, 'accel': accel_trig,\n",
    "                  'ed': en_den_trig, 'heating': heating_trig, 'turning': turning_trig, 'res_heat': residual_heat_trig})\n",
    "\n",
    "\n",
    "\n",
    "# pp_normalized = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33946753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the labels excel files\n",
    "label = pd.read_excel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea18517",
   "metadata": {},
   "source": [
    "### Now cycle through each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in tqdm(range(2,251)):\n",
    "    if ii < 10:\n",
    "        layer_text = 'layer000' + str(ii)\n",
    "    elif ii < 100:\n",
    "        layer_text = 'layer00' + str(ii)\n",
    "    else:\n",
    "        layer_text = 'layer0' + str(ii)\n",
    "        \n",
    "    data = pd.read_csv(DATA_DIR + 'T500_3D_Scan_Strategies_fused_' + layer_text + '.csv', header=None, \n",
    "                       names=['X', 'Y', 'P', 'T', 'deprecated'])\n",
    "    \n",
    "    trigger_indx = np.where(np.array(data['T']))[0]\n",
    "    X = np.array(data['X'])\n",
    "    Y = np.array(data['Y'])\n",
    "    P = np.array(data['P'])\n",
    "    \n",
    "    # Speed\n",
    "    x_vel = np.zeros(X.shape)\n",
    "    x_vel[1:-1] = (X[2:]-X[:-2]) / 2 / dt\n",
    "\n",
    "    y_vel = np.zeros(X.shape)\n",
    "    y_vel[1:-1] = (Y[2:]-Y[:-2]) / 2 / dt\n",
    "\n",
    "    speed = np.sqrt(np.square(x_vel) + np.square(y_vel))\n",
    "    \n",
    "    # Direction\n",
    "    vel_dir_x = x_vel / (speed + 1e-8)\n",
    "    vel_dir_y = y_vel / (speed + 1e-8)\n",
    "    \n",
    "    # Acceleration\n",
    "    x_accel = np.zeros(X.shape)\n",
    "    x_accel[1:-1] = (x_vel[2:]-x_vel[:-2]) / 2 / dt\n",
    "\n",
    "    y_accel = np.zeros(Y.shape)\n",
    "    y_accel[1:-1] = (y_vel[2:]-y_vel[:-2]) / 2 / dt\n",
    "\n",
    "    accel = np.sqrt(np.square(x_accel) + np.square(y_accel))\n",
    "    accel[np.where(speed[1:] < speed[:-1])[0]+1] = accel[np.where(speed[1:] < speed[:-1])[0]+1]*-1\n",
    "    \n",
    "    # Energy Density\n",
    "    en_den = P / (speed + 1e-8)\n",
    "    \n",
    "    # Heating/Cooling\n",
    "    heating = np.zeros(X.shape)\n",
    "    heating[np.where(P[1:]>P[:-1]+1)[0]+1] = 1\n",
    "    heating[np.where(P[1:]<P[:-1]-1)[0]+1] = -1\n",
    "    \n",
    "    # Turning\n",
    "    turn_threshold = 0.01 # Tune me\n",
    "\n",
    "    turning = np.zeros(X.shape)\n",
    "    turn1 = np.zeros(X.shape)\n",
    "    turn2 = np.zeros(X.shape)\n",
    "\n",
    "    turn1[1:] = np.abs(vel_dir_x[1:]-vel_dir_x[:-1])\n",
    "    turn2[1:] = np.abs(vel_dir_y[1:]-vel_dir_y[:-1])\n",
    "    turning = np.any([(turn1 > turn_threshold), (turn2 > turn_threshold)], axis=0)\n",
    "    turning = turning.astype('int')\n",
    "    \n",
    "    # Residual Heat\n",
    "    R = 10 # parts are 10 mm x 10 mm\n",
    "    T = 5\n",
    "    max_iters = round(T/dt)\n",
    "\n",
    "    residual_heat = np.zeros(X.shape[0])\n",
    "    for i in trigger_indx:\n",
    "        d_ij = np.sqrt(np.square(X[i]-X[max(0, i-max_iters):i]) + np.square(Y[i]-Y[max(0, i-max_iters):i]))\n",
    "        t_ij = np.linspace((min(max_iters, i))*dt, dt, min(max_iters, i))\n",
    "        p_j = P[max(0, i-max_iters):i]\n",
    "        residual_heat[i] += np.sum(np.square((R-d_ij)/R) * ((T-t_ij)/T) * p_j)\n",
    "        \n",
    "    P_trig = P[trigger_indx]\n",
    "    speed_trig = speed[trigger_indx]\n",
    "    x_dir_trig = vel_dir_x[trigger_indx]\n",
    "    y_dir_trig = vel_dir_y[trigger_indx]\n",
    "    accel_trig = accel[trigger_indx]\n",
    "    en_den_trig = en_den[trigger_indx]\n",
    "    heating_trig = heating[trigger_indx]\n",
    "    turning_trig = turning[trigger_indx]\n",
    "    residual_heat_trig = residual_heat[trigger_indx]\n",
    "\n",
    "    pp = pd.DataFrame({'P': P_trig, 'speed': speed_trig, 'x_dir': x_dir_trig, 'y_dir': y_dir_trig, 'accel': accel_trig,\n",
    "                      'ed': en_den_trig, 'heating': heating_trig, 'turning': turning_trig, 'res_heat': residual_heat_trig})\n",
    "    \n",
    "    pp.to_excel('process_parameters/by_layer/' + layer_text + '.xlsx',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13104ea",
   "metadata": {},
   "source": [
    "### Cycle Through RTC/Base Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in tqdm(('Base', 'MPA')):\n",
    "    XYPT_File = DATA_DIR + 'RTC_XYL_DAQ_IN625_Ar_P01I2000D10_RefA400_' + ii + '.csv'\n",
    "        \n",
    "    data = pd.read_csv(XYPT_File, header=None, names=['X', 'Y', 'P', 'T'])\n",
    "    \n",
    "    trigger_indx = np.where(np.array(data['T']))[0]\n",
    "    X = np.array(data['X'])\n",
    "    Y = np.array(data['Y'])\n",
    "    P = np.array(data['P'])\n",
    "    \n",
    "    # Speed\n",
    "    x_vel = np.zeros(X.shape)\n",
    "    x_vel[1:-1] = (X[2:]-X[:-2]) / 2 / dt\n",
    "\n",
    "    y_vel = np.zeros(X.shape)\n",
    "    y_vel[1:-1] = (Y[2:]-Y[:-2]) / 2 / dt\n",
    "\n",
    "    speed = np.sqrt(np.square(x_vel) + np.square(y_vel))\n",
    "    \n",
    "    # Direction\n",
    "    vel_dir_x = x_vel / (speed + 1e-8)\n",
    "    vel_dir_y = y_vel / (speed + 1e-8)\n",
    "    \n",
    "    # Acceleration\n",
    "    x_accel = np.zeros(X.shape)\n",
    "    x_accel[1:-1] = (x_vel[2:]-x_vel[:-2]) / 2 / dt\n",
    "\n",
    "    y_accel = np.zeros(Y.shape)\n",
    "    y_accel[1:-1] = (y_vel[2:]-y_vel[:-2]) / 2 / dt\n",
    "\n",
    "    accel = np.sqrt(np.square(x_accel) + np.square(y_accel))\n",
    "    accel[np.where(speed[1:] < speed[:-1])[0]+1] = accel[np.where(speed[1:] < speed[:-1])[0]+1]*-1\n",
    "    \n",
    "    # Energy Density\n",
    "    en_den = P / (speed + 1e-8)\n",
    "    \n",
    "    # Heating/Cooling\n",
    "    heating = np.zeros(X.shape)\n",
    "    heating[np.where(P[1:]>P[:-1]+1)[0]+1] = 1\n",
    "    heating[np.where(P[1:]<P[:-1]-1)[0]+1] = -1\n",
    "    \n",
    "    # Turning\n",
    "    turn_threshold = 0.01 # Tune me\n",
    "\n",
    "    turning = np.zeros(X.shape)\n",
    "    turn1 = np.zeros(X.shape)\n",
    "    turn2 = np.zeros(X.shape)\n",
    "\n",
    "    turn1[1:] = np.abs(vel_dir_x[1:]-vel_dir_x[:-1])\n",
    "    turn2[1:] = np.abs(vel_dir_y[1:]-vel_dir_y[:-1])\n",
    "    turning = np.any([(turn1 > turn_threshold), (turn2 > turn_threshold)], axis=0)\n",
    "    turning = turning.astype('int')\n",
    "    \n",
    "    # Residual Heat\n",
    "    R = 10 # parts are 10 mm x 10 mm\n",
    "    T = 5\n",
    "    max_iters = round(T/dt)\n",
    "\n",
    "    residual_heat = np.zeros(X.shape[0])\n",
    "    for i in trigger_indx:\n",
    "        d_ij = np.sqrt(np.square(X[i]-X[max(0, i-max_iters):i]) + np.square(Y[i]-Y[max(0, i-max_iters):i]))\n",
    "        t_ij = np.linspace((min(max_iters, i))*dt, dt, min(max_iters, i))\n",
    "        p_j = P[max(0, i-max_iters):i]\n",
    "        residual_heat[i] += np.sum(np.square((R-d_ij)/R) * ((T-t_ij)/T) * p_j)\n",
    "        \n",
    "    P_trig = P[trigger_indx]\n",
    "    speed_trig = speed[trigger_indx]\n",
    "    x_dir_trig = vel_dir_x[trigger_indx]\n",
    "    y_dir_trig = vel_dir_y[trigger_indx]\n",
    "    accel_trig = accel[trigger_indx]\n",
    "    en_den_trig = en_den[trigger_indx]\n",
    "    heating_trig = heating[trigger_indx]\n",
    "    turning_trig = turning[trigger_indx]\n",
    "    residual_heat_trig = residual_heat[trigger_indx]\n",
    "\n",
    "    pp = pd.DataFrame({'P': P_trig, 'speed': speed_trig, 'x_dir': x_dir_trig, 'y_dir': y_dir_trig, 'accel': accel_trig,\n",
    "                      'ed': en_den_trig, 'heating': heating_trig, 'turning': turning_trig, 'res_heat': residual_heat_trig})\n",
    "    \n",
    "    pp.to_excel('process_parameters/RTC/' + ii + '.xlsx',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17c9dc",
   "metadata": {},
   "source": [
    "### Remove outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in tqdm(range(2,251)):\n",
    "    \n",
    "    if ii < 10:\n",
    "        layer_text = 'layer000' + str(ii)\n",
    "    elif ii < 100:\n",
    "        layer_text = 'layer00' + str(ii)\n",
    "    else:\n",
    "        layer_text = 'layer0' + str(ii)\n",
    "    \n",
    "    # Remove Energy Density outliers\n",
    "    pp = pd.read_excel('process_parameters/by_layer/' + layer_text + '.xlsx', header=None)\n",
    "    new_ed = np.array(pp[5])\n",
    "    new_ed[np.where(new_ed>1)] = 1\n",
    "    pp[5] = new_ed\n",
    "    \n",
    "    # Save files\n",
    "    pp.to_excel('process_parameters/by_layer/no_outliers/' + layer_text + '.xlsx',header=False, index=False)\n",
    "\n",
    "\n",
    "# for ii in tqdm(('Base', 'MPA')):\n",
    "    \n",
    "#     # Remove Energy Density outliers\n",
    "#     pp = pd.read_excel('process_parameters/RTC/' + ii + '.xlsx', header=None)\n",
    "#     new_ed = np.array(pp[5])\n",
    "#     new_ed[np.where(new_ed>1)] = 1\n",
    "#     pp[5] = new_ed\n",
    "    \n",
    "#     # Save files\n",
    "#     pp.to_excel('process_parameters/RTC/no_outliers/' + ii + '.xlsx',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed3e61",
   "metadata": {},
   "source": [
    "### Normalize data for mean=0, std=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db69316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of data\n",
    "num_data = 0\n",
    "for ii in tqdm(range(2,251)):\n",
    "    \n",
    "    if ii < 10:\n",
    "        layer_text = 'layer000' + str(ii)\n",
    "    elif ii < 100:\n",
    "        layer_text = 'layer00' + str(ii)\n",
    "    else:\n",
    "        layer_text = 'layer0' + str(ii)\n",
    "    \n",
    "    pp = pd.read_excel('process_parameters/by_layer/no_outliers/' + layer_text + '.xlsx', header=None)\n",
    "    num_data += pp[0].shape[0]\n",
    "    \n",
    "print(\"The total number of data points is: \" + str(num_data))\n",
    "\n",
    "# num_data = 0\n",
    "# for ii in tqdm(('Base', 'MPA')):\n",
    "    \n",
    "#     pp = pd.read_excel('process_parameters/RTC/no_outliers/' + ii + '.xlsx', header=None)\n",
    "#     num_data += pp[0].shape[0]\n",
    "    \n",
    "# print(\"The total number of data points is: \" + str(num_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_total        = np.zeros((num_data,1))\n",
    "speed_total    = np.zeros((num_data,1))\n",
    "x_dir_total    = np.zeros((num_data,1))\n",
    "y_dir_total    = np.zeros((num_data,1))\n",
    "accel_total    = np.zeros((num_data,1))\n",
    "ed_total       = np.zeros((num_data,1))\n",
    "heating_total  = np.zeros((num_data,1))\n",
    "turning_total  = np.zeros((num_data,1))\n",
    "res_heat_total = np.zeros((num_data,1))\n",
    "\n",
    "# Now collect all the data\n",
    "indx = 0\n",
    "for ii in tqdm(range(2,251)):\n",
    "# for ii in tqdm(('Base', 'MPA')):\n",
    "    \n",
    "    if ii < 10:\n",
    "        layer_text = 'layer000' + str(ii)\n",
    "    elif ii < 100:\n",
    "        layer_text = 'layer00' + str(ii)\n",
    "    else:\n",
    "        layer_text = 'layer0' + str(ii)\n",
    "    \n",
    "    pp = pd.read_excel('process_parameters/by_layer/no_outliers/' + layer_text + '.xlsx', header=None)\n",
    "#     pp = pd.read_excel('process_parameters/RTC/no_outliers/' + ii + '.xlsx', header=None)\n",
    "\n",
    "    \n",
    "    data_in_layer = pp[0].shape[0]\n",
    "    \n",
    "    P = np.array(pp[0])\n",
    "    speed = np.array(pp[1])\n",
    "    x_dir = np.array(pp[2])\n",
    "    y_dir = np.array(pp[3])\n",
    "    accel = np.array(pp[4])\n",
    "    ed = np.array(pp[5])\n",
    "    heating = np.array(pp[6])\n",
    "    turning = np.array(pp[7])\n",
    "    res_heat = np.array(pp[8])\n",
    "    \n",
    "    new_indx = indx+data_in_layer\n",
    "    \n",
    "    P_total[indx:new_indx,0]        = P\n",
    "    speed_total[indx:new_indx,0]    = speed\n",
    "    x_dir_total[indx:new_indx,0]    = x_dir\n",
    "    y_dir_total[indx:new_indx,0]    = y_dir\n",
    "    accel_total[indx:new_indx,0]    = accel\n",
    "    ed_total[indx:new_indx,0]       = ed\n",
    "    heating_total[indx:new_indx,0]  = heating\n",
    "    turning_total[indx:new_indx,0]  = turning\n",
    "    res_heat_total[indx:new_indx,0] = res_heat\n",
    "    \n",
    "    indx = new_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cedd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalizing coefficients\n",
    "pp_mean = dict()\n",
    "pp_std = dict()\n",
    "\n",
    "pp_mean['P']        = np.mean(P_total)\n",
    "pp_mean['speed']    = np.mean(speed_total)\n",
    "pp_mean['x_dir']    = np.mean(x_dir_total)\n",
    "pp_mean['y_dir']    = np.mean(y_dir_total)\n",
    "pp_mean['accel']    = np.mean(accel_total)\n",
    "pp_mean['ed']       = np.mean(ed_total)\n",
    "pp_mean['heating']  = np.mean(heating_total)\n",
    "pp_mean['turning']  = np.mean(turning_total)\n",
    "pp_mean['res_heat'] = np.mean(res_heat_total)\n",
    "\n",
    "pp_std['P']        = np.std(P_total)\n",
    "pp_std['speed']    = np.std(speed_total)\n",
    "pp_std['x_dir']    = np.std(x_dir_total)\n",
    "pp_std['y_dir']    = np.std(y_dir_total)\n",
    "pp_std['accel']    = np.std(accel_total)\n",
    "pp_std['ed']       = np.std(ed_total)\n",
    "pp_std['heating']  = np.std(heating_total)\n",
    "pp_std['turning']  = np.std(turning_total)\n",
    "pp_std['res_heat'] = np.std(res_heat_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "P_norm        = np.squeeze((P_total - pp_mean['P']) / pp_std['P'])\n",
    "speed_norm    = np.squeeze((speed_total - pp_mean['speed']) / pp_std['speed'])\n",
    "x_dir_norm    = np.squeeze((x_dir_total - pp_mean['x_dir']) / pp_std['x_dir'])\n",
    "y_dir_norm    = np.squeeze((y_dir_total - pp_mean['y_dir']) / pp_std['y_dir'])\n",
    "accel_norm    = np.squeeze((accel_total - pp_mean['accel']) / pp_std['accel'])\n",
    "ed_norm       = np.squeeze((ed_total - pp_mean['ed']) / pp_std['ed'])\n",
    "heating_norm  = np.squeeze(heating_total) # boolean values\n",
    "turning_norm  = np.squeeze(turning_total) # boolean values\n",
    "res_heat_norm = np.squeeze((res_heat_total - pp_mean['res_heat']) / pp_std['res_heat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676aa65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = list()\n",
    "for ii in tqdm(range(2,251)):\n",
    "# for ii in tqdm(('Base', 'MPA')):\n",
    "\n",
    "    if ii < 10:\n",
    "        layer_text = 'layer000' + str(ii)\n",
    "    elif ii < 100:\n",
    "        layer_text = 'layer00' + str(ii)\n",
    "    else:\n",
    "        layer_text = 'layer0' + str(ii)\n",
    "        \n",
    "    pp = pd.read_excel('process_parameters/by_layer/no_outliers/' + layer_text + '.xlsx', header=None)\n",
    "#     pp = pd.read_excel('process_parameters/RTC/no_outliers/' + ii + '.xlsx', header=None)\n",
    "    data_in_layer = pp[0].shape[0]\n",
    "    \n",
    "    for image_num in range(data_in_layer):\n",
    "        image_name.append('layer' + str(ii) + '_' + str(image_num+1) + '.png')\n",
    "#         image_name.append(ii + '_' + str(image_num+1) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the normalized process parameters to a spreadsheet\n",
    "pp_norm = pd.DataFrame({'P': P_norm, 'speed': speed_norm, 'x_dir': x_dir_norm, 'y_dir': y_dir_norm, 'accel': accel_norm,\n",
    "                      'ed': ed_norm, 'heating': heating_norm, 'turning': turning_norm, 'res_heat': res_heat_norm,\n",
    "                       'image_name': np.array(image_name)})\n",
    "\n",
    "pp_norm.to_excel('process_parameters/normalized_pp_no_outliers.xlsx',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b929d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_norm = pd.read_excel('process_parameters/normalized_pp_no_outliers.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fe907",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60117cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = pp_norm[9]\n",
    "stripe = np.zeros(len(image_name))\n",
    "concentric = np.zeros(len(image_name))\n",
    "island = np.zeros(len(image_name))\n",
    "for ii in tqdm(range(len(image_name))):\n",
    "    name = image_name[ii]\n",
    "    underscore = name.find('_')\n",
    "    layer_type = int(name[5:underscore]) % 12\n",
    "    if layer_type == 1:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0 \n",
    "        island[ii] = 0\n",
    "    elif layer_type == 2:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0 \n",
    "        island[ii] = 1\n",
    "    elif layer_type == 3:\n",
    "        stripe[ii] = 0\n",
    "        concentric[ii] = 1\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 4:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0 \n",
    "        island[ii] = 1\n",
    "    elif layer_type == 5:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 6:\n",
    "        stripe[ii] = 0\n",
    "        concentric[ii] = 1\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 7:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 8:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 9:\n",
    "        stripe[ii] = 0\n",
    "        concentric[ii] = 1\n",
    "        island[ii] = 0\n",
    "    elif layer_type == 10:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 11:\n",
    "        stripe[ii] = 1\n",
    "        concentric[ii] = 0\n",
    "        island[ii] = 1\n",
    "    elif layer_type == 0:\n",
    "        stripe[ii] = 0\n",
    "        concentric[ii] = 1\n",
    "        island[ii] = 0\n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff96a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp_norm.insert(9, 'stripe', stripe)\n",
    "pp_norm.insert(10, 'concentric', concentric)\n",
    "pp_norm.insert(11, 'island', island)\n",
    "pp_norm.to_excel('process_parameters/normalized_pp_no_outliers_with_scanstrategy.xlsx',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282a389",
   "metadata": {},
   "source": [
    "### Create a unified spreadsheet holding image name, labels, and process parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071152f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_norm = pd.read_excel('process_parameters/normalized_pp_no_outliers_with_scanstrategy.xlsx', header=None)\n",
    "train_labels = pd.read_excel('neural_network_data/train_labels_3d_8.xlsx', header=None)\n",
    "dev_labels = pd.read_excel('neural_network_data/dev_labels_3d_8.xlsx', header=None)\n",
    "test_labels = pd.read_excel('neural_network_data/test_labels_3d_8.xlsx', header=None)\n",
    "\n",
    "# pp_norm = pd.read_excel('process_parameters/normalized_pp_no_outliers_RTC.xlsx', header=None)\n",
    "# train_labels = pd.read_excel('neural_network_data/train_labels_RTC6.xlsx', header=None)\n",
    "# dev_labels = pd.read_excel('neural_network_data/dev_labels_RTC6.xlsx', header=None)\n",
    "# test_labels = pd.read_excel('neural_network_data/test_labels_RTC6.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddafe16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.079523</td>\n",
       "      <td>-3.676217</td>\n",
       "      <td>-0.980215</td>\n",
       "      <td>-1.007752</td>\n",
       "      <td>0.205582</td>\n",
       "      <td>3.002867</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.780827</td>\n",
       "      <td>Base_1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.079523</td>\n",
       "      <td>-3.644069</td>\n",
       "      <td>-0.741498</td>\n",
       "      <td>-1.197503</td>\n",
       "      <td>0.303601</td>\n",
       "      <td>2.841857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.780495</td>\n",
       "      <td>Base_2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.079523</td>\n",
       "      <td>-3.644998</td>\n",
       "      <td>-0.454760</td>\n",
       "      <td>-1.336256</td>\n",
       "      <td>0.322030</td>\n",
       "      <td>2.846391</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.780165</td>\n",
       "      <td>Base_3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.080580</td>\n",
       "      <td>-3.645153</td>\n",
       "      <td>-0.143848</td>\n",
       "      <td>-1.407322</td>\n",
       "      <td>-0.330548</td>\n",
       "      <td>2.845673</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.779835</td>\n",
       "      <td>Base_4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.079523</td>\n",
       "      <td>-3.548126</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>-1.416120</td>\n",
       "      <td>0.014308</td>\n",
       "      <td>2.408366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.779507</td>\n",
       "      <td>Base_5.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41801</th>\n",
       "      <td>0.597365</td>\n",
       "      <td>-2.873533</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>1.413668</td>\n",
       "      <td>-0.559515</td>\n",
       "      <td>4.529382</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.682967</td>\n",
       "      <td>MPA_20899.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41802</th>\n",
       "      <td>0.855704</td>\n",
       "      <td>-3.150116</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>1.413668</td>\n",
       "      <td>-0.119584</td>\n",
       "      <td>5.757255</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.687807</td>\n",
       "      <td>MPA_20900.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41803</th>\n",
       "      <td>0.726006</td>\n",
       "      <td>-3.226036</td>\n",
       "      <td>0.183239</td>\n",
       "      <td>1.403451</td>\n",
       "      <td>-0.586557</td>\n",
       "      <td>5.757255</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.692232</td>\n",
       "      <td>MPA_20901.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41804</th>\n",
       "      <td>-8.323260</td>\n",
       "      <td>-3.229389</td>\n",
       "      <td>0.593891</td>\n",
       "      <td>1.289046</td>\n",
       "      <td>-0.560832</td>\n",
       "      <td>-3.071179</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.696698</td>\n",
       "      <td>MPA_20902.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41805</th>\n",
       "      <td>-8.323260</td>\n",
       "      <td>-3.224917</td>\n",
       "      <td>0.951793</td>\n",
       "      <td>1.057408</td>\n",
       "      <td>0.551193</td>\n",
       "      <td>-3.071179</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.700214</td>\n",
       "      <td>MPA_20903.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41806 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5  6  7  \\\n",
       "0     -4.079523 -3.676217 -0.980215 -1.007752  0.205582  3.002867  1  0   \n",
       "1     -4.079523 -3.644069 -0.741498 -1.197503  0.303601  2.841857  0  1   \n",
       "2     -4.079523 -3.644998 -0.454760 -1.336256  0.322030  2.846391  0  1   \n",
       "3     -4.080580 -3.645153 -0.143848 -1.407322 -0.330548  2.845673  0  1   \n",
       "4     -4.079523 -3.548126  0.013606 -1.416120  0.014308  2.408366  0  0   \n",
       "...         ...       ...       ...       ...       ...       ... .. ..   \n",
       "41801  0.597365 -2.873533  0.013606  1.413668 -0.559515  4.529382 -1  0   \n",
       "41802  0.855704 -3.150116  0.013606  1.413668 -0.119584  5.757255  1  0   \n",
       "41803  0.726006 -3.226036  0.183239  1.403451 -0.586557  5.757255 -1  1   \n",
       "41804 -8.323260 -3.229389  0.593891  1.289046 -0.560832 -3.071179 -1  1   \n",
       "41805 -8.323260 -3.224917  0.951793  1.057408  0.551193 -3.071179  0  1   \n",
       "\n",
       "              8              9  \n",
       "0     -1.780827     Base_1.png  \n",
       "1     -1.780495     Base_2.png  \n",
       "2     -1.780165     Base_3.png  \n",
       "3     -1.779835     Base_4.png  \n",
       "4     -1.779507     Base_5.png  \n",
       "...         ...            ...  \n",
       "41801  2.682967  MPA_20899.png  \n",
       "41802  2.687807  MPA_20900.png  \n",
       "41803  2.692232  MPA_20901.png  \n",
       "41804  2.696698  MPA_20902.png  \n",
       "41805  2.700214  MPA_20903.png  \n",
       "\n",
       "[41806 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b144725",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/meltpool_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/meltpool_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/meltpool_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 12",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m indx_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mpp_norm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m))):\n\u001b[1;32m      3\u001b[0m     indx_dict[pp_norm[\u001b[38;5;241m12\u001b[39m][ii]] \u001b[38;5;241m=\u001b[39m ii\n",
      "File \u001b[0;32m~/miniconda3/envs/meltpool_env/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/meltpool_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 12"
     ]
    }
   ],
   "source": [
    "indx_dict = dict()\n",
    "for ii in tqdm(range(len(pp_norm[12]))):\n",
    "    indx_dict[pp_norm[12][ii]] = ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = '../../../In-situ Meas Data/In-situ Meas Data/Melt Pool Camera Preprocessed PNG/'\n",
    "# image_dir = '../../Melt Pool Camera Preprocessed PNG/'\n",
    "\n",
    "image_name_list = np.array(pp_norm[12])\n",
    "\n",
    "# Train set\n",
    "P_train        = list()\n",
    "speed_train    = list()\n",
    "x_dir_train    = list()\n",
    "y_dir_train    = list()\n",
    "accel_train    = list()\n",
    "ed_train       = list()\n",
    "heating_train  = list()\n",
    "turning_train  = list()\n",
    "res_heat_train = list()\n",
    "stripe_train   = list()\n",
    "concentric_train = list()\n",
    "island_train = list()\n",
    "image_name_train = list()\n",
    "label_train = list()\n",
    "\n",
    "for ii in tqdm(range(train_labels[0].shape[0])):\n",
    "    image_loc = train_labels[0][ii]\n",
    "    image_name_start = image_loc.find('layer', 80)\n",
    "    if image_name_start == -1:\n",
    "        raise Exception(\"Couldn't find image name\")\n",
    "        \n",
    "    image_name = image_loc[image_name_start:]\n",
    "    \n",
    "    underscore = image_name.find('_')\n",
    "    layer_number = int(image_name[5:underscore])\n",
    "    \n",
    "    if layer_number != 1 and layer_number != 5: # we do not have the data for layer1 and layer5 for some reason\n",
    "\n",
    "        image_name_indx = indx_dict[image_name]\n",
    "        \n",
    "        P_train.append(pp_norm[0][image_name_indx])\n",
    "        speed_train.append(pp_norm[1][image_name_indx])\n",
    "        x_dir_train.append(pp_norm[2][image_name_indx])\n",
    "        y_dir_train.append(pp_norm[3][image_name_indx])\n",
    "        accel_train.append(pp_norm[4][image_name_indx])\n",
    "        ed_train.append(pp_norm[5][image_name_indx])\n",
    "        heating_train.append(pp_norm[6][image_name_indx])\n",
    "        turning_train.append(pp_norm[7][image_name_indx])\n",
    "        res_heat_train.append(pp_norm[8][image_name_indx])\n",
    "        stripe_train.append(pp_norm[9][image_name_indx])\n",
    "        concentric_train.append(pp_norm[10][image_name_indx])\n",
    "        island_train.append(pp_norm[11][image_name_indx])\n",
    "        image_name_train.append(image_name)\n",
    "        label_train.append(train_labels[1][ii])\n",
    "\n",
    "train_labels_pp = pd.DataFrame({'image_name': np.array(image_name_train), 'label': np.array(label_train),\n",
    "                               'P': np.array(P_train), 'speed': np.array(speed_train), 'x_dir': np.array(x_dir_train), \n",
    "                               'y_dir': np.array(y_dir_train), 'accel': np.array(accel_train), 'ed': np.array(ed_train), \n",
    "                               'heating': np.array(heating_train), 'turning': np.array(turning_train), \n",
    "                               'res_heat': np.array(res_heat_train)})\n",
    "\n",
    "train_labels_pp_limited = pd.DataFrame({'image_name': np.array(image_name_train), 'label': np.array(label_train),\n",
    "                               'P': np.array(P_train), 'speed': np.array(speed_train), 'accel': np.array(accel_train), \n",
    "                              'ed': np.array(ed_train), 'heating': np.array(heating_train), 'res_heat': np.array(res_heat_train)})\n",
    "\n",
    "train_labels_pp_limited_scan = pd.DataFrame({'image_name': np.array(image_name_train), 'label': np.array(label_train),\n",
    "                               'P': np.array(P_train), 'speed': np.array(speed_train), 'accel': np.array(accel_train), \n",
    "                              'ed': np.array(ed_train), 'heating': np.array(heating_train), 'res_heat': np.array(res_heat_train),\n",
    "                              'stripe': np.array(stripe_train), 'concentric': np.array(concentric_train), 'island': np.array(island_train)})\n",
    "                                     \n",
    "train_labels_pp.to_csv('neural_network_data/train_labels_pp_3d_8.csv',header=True, index=False)\n",
    "train_labels_pp_limited.to_csv('neural_network_data/train_labels_pp_limited_3d_8.csv',header=True, index=False)\n",
    "train_labels_pp_limited_scan.to_csv('neural_network_data/train_labels_pp_limited_scan_3d_8.csv',header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d82d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev set\n",
    "P_dev        = list()\n",
    "speed_dev    = list()\n",
    "x_dir_dev    = list()\n",
    "y_dir_dev    = list()\n",
    "accel_dev    = list()\n",
    "ed_dev       = list()\n",
    "heating_dev  = list()\n",
    "turning_dev  = list()\n",
    "res_heat_dev = list()\n",
    "stripe_dev   = list()\n",
    "concentric_dev = list()\n",
    "island_dev = list()\n",
    "image_name_dev = list()\n",
    "label_dev = list()\n",
    "\n",
    "for ii in tqdm(range(dev_labels[0].shape[0])):\n",
    "    image_loc = dev_labels[0][ii]\n",
    "    image_name_start = image_loc.find('layer', 80)\n",
    "    if image_name_start == -1:\n",
    "        raise Exception(\"Couldn't find image name\")\n",
    "        \n",
    "    image_name = image_loc[image_name_start:]\n",
    "    \n",
    "    underscore = image_name.find('_')\n",
    "    layer_number = int(image_name[5:underscore])\n",
    "    \n",
    "    if layer_number != 1 and layer_number != 5: # we do not have the data for layer1 and layer5 for some reason\n",
    "#         image_name_indx = np.where(image_name_list == image_name)[0][0]\n",
    "\n",
    "        image_name_indx = indx_dict[image_name]\n",
    "        \n",
    "        P_dev.append(pp_norm[0][image_name_indx])\n",
    "        speed_dev.append(pp_norm[1][image_name_indx])\n",
    "        x_dir_dev.append(pp_norm[2][image_name_indx])\n",
    "        y_dir_dev.append(pp_norm[3][image_name_indx])\n",
    "        accel_dev.append(pp_norm[4][image_name_indx])\n",
    "        ed_dev.append(pp_norm[5][image_name_indx])\n",
    "        heating_dev.append(pp_norm[6][image_name_indx])\n",
    "        turning_dev.append(pp_norm[7][image_name_indx])\n",
    "        res_heat_dev.append(pp_norm[8][image_name_indx])\n",
    "        stripe_dev.append(pp_norm[9][image_name_indx])\n",
    "        concentric_dev.append(pp_norm[10][image_name_indx])\n",
    "        island_dev.append(pp_norm[11][image_name_indx])\n",
    "        image_name_dev.append(image_name)\n",
    "        label_dev.append(dev_labels[1][ii])\n",
    "\n",
    "dev_labels_pp = pd.DataFrame({'image_name': np.array(image_name_dev), 'label': np.array(label_dev),\n",
    "                               'P': np.array(P_dev), 'speed': np.array(speed_dev), 'x_dir': np.array(x_dir_dev), \n",
    "                               'y_dir': np.array(y_dir_dev), 'accel': np.array(accel_dev), 'ed': np.array(ed_dev), \n",
    "                               'heating': np.array(heating_dev), 'turning': np.array(turning_dev), \n",
    "                               'res_heat': np.array(res_heat_dev)})\n",
    "\n",
    "dev_labels_pp_limited = pd.DataFrame({'image_name': np.array(image_name_dev), 'label': np.array(label_dev),\n",
    "                               'P': np.array(P_dev), 'speed': np.array(speed_dev), 'accel': np.array(accel_dev), \n",
    "                              'ed': np.array(ed_dev), 'heating': np.array(heating_dev), 'res_heat': np.array(res_heat_dev)})\n",
    "\n",
    "dev_labels_pp_limited_scan = pd.DataFrame({'image_name': np.array(image_name_dev), 'label': np.array(label_dev),\n",
    "                               'P': np.array(P_dev), 'speed': np.array(speed_dev), 'accel': np.array(accel_dev), \n",
    "                              'ed': np.array(ed_dev), 'heating': np.array(heating_dev), 'res_heat': np.array(res_heat_dev),\n",
    "                              'stripe': np.array(stripe_dev), 'concentric': np.array(concentric_dev), 'island': np.array(island_dev)})\n",
    "                                     \n",
    "dev_labels_pp.to_csv('neural_network_data/dev_labels_pp_3d_8.csv',header=True, index=False)\n",
    "dev_labels_pp_limited.to_csv('neural_network_data/dev_labels_pp_limited_3d_8.csv',header=True, index=False)\n",
    "dev_labels_pp_limited_scan.to_csv('neural_network_data/dev_labels_pp_limited_scan_3d_8.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d5a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "P_test        = list()\n",
    "speed_test    = list()\n",
    "x_dir_test    = list()\n",
    "y_dir_test    = list()\n",
    "accel_test    = list()\n",
    "ed_test       = list()\n",
    "heating_test  = list()\n",
    "turning_test  = list()\n",
    "res_heat_test = list()\n",
    "stripe_test   = list()\n",
    "concentric_test = list()\n",
    "island_test = list()\n",
    "image_name_test = list()\n",
    "label_test = list()\n",
    "\n",
    "for ii in tqdm(range(test_labels[0].shape[0])):\n",
    "    image_loc = test_labels[0][ii]\n",
    "    image_name_start = image_loc.find('layer', 80)\n",
    "    if image_name_start == -1:\n",
    "        raise Exception(\"Couldn't find image name\")\n",
    "        \n",
    "    image_name = image_loc[image_name_start:]\n",
    "    \n",
    "    underscore = image_name.find('_')\n",
    "    layer_number = int(image_name[5:underscore])\n",
    "    \n",
    "    if layer_number != 1 and layer_number != 5: # we do not have the data for layer1 and layer5 for some reason\n",
    "#         image_name_indx = np.where(image_name_list == image_name)[0][0]\n",
    "\n",
    "        image_name_indx = indx_dict[image_name]\n",
    "        \n",
    "        P_test.append(pp_norm[0][image_name_indx])\n",
    "        speed_test.append(pp_norm[1][image_name_indx])\n",
    "        x_dir_test.append(pp_norm[2][image_name_indx])\n",
    "        y_dir_test.append(pp_norm[3][image_name_indx])\n",
    "        accel_test.append(pp_norm[4][image_name_indx])\n",
    "        ed_test.append(pp_norm[5][image_name_indx])\n",
    "        heating_test.append(pp_norm[6][image_name_indx])\n",
    "        turning_test.append(pp_norm[7][image_name_indx])\n",
    "        res_heat_test.append(pp_norm[8][image_name_indx])\n",
    "        stripe_test.append(pp_norm[9][image_name_indx])\n",
    "        concentric_test.append(pp_norm[10][image_name_indx])\n",
    "        island_test.append(pp_norm[11][image_name_indx])\n",
    "        image_name_test.append(image_name)\n",
    "        label_test.append(test_labels[1][ii])\n",
    "\n",
    "test_labels_pp = pd.DataFrame({'image_name': np.array(image_name_test), 'label': np.array(label_test),\n",
    "                               'P': np.array(P_test), 'speed': np.array(speed_test), 'x_dir': np.array(x_dir_test), \n",
    "                               'y_dir': np.array(y_dir_test), 'accel': np.array(accel_test), 'ed': np.array(ed_test), \n",
    "                               'heating': np.array(heating_test), 'turning': np.array(turning_test), \n",
    "                               'res_heat': np.array(res_heat_test)})\n",
    "                                     \n",
    "test_labels_pp_limited = pd.DataFrame({'image_name': np.array(image_name_test), 'label': np.array(label_test),\n",
    "                               'P': np.array(P_test), 'speed': np.array(speed_test), 'accel': np.array(accel_test), \n",
    "                              'ed': np.array(ed_test), 'heating': np.array(heating_test), 'res_heat': np.array(res_heat_test)})\n",
    "\n",
    "test_labels_pp_limited_scan = pd.DataFrame({'image_name': np.array(image_name_test), 'label': np.array(label_test),\n",
    "                               'P': np.array(P_test), 'speed': np.array(speed_test), 'accel': np.array(accel_test), \n",
    "                              'ed': np.array(ed_test), 'heating': np.array(heating_test), 'res_heat': np.array(res_heat_test),\n",
    "                              'stripe': np.array(stripe_test), 'concentric': np.array(concentric_test), 'island': np.array(island_test)})\n",
    "\n",
    "test_labels_pp.to_csv('neural_network_data/test_labels_pp_3d_8.csv',header=True, index=False)\n",
    "test_labels_pp_limited.to_csv('neural_network_data/test_labels_pp_limited_3d_8.csv',header=True, index=False)\n",
    "test_labels_pp_limited_scan.to_csv('neural_network_data/test_labels_pp_limited_scan_3d_8.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTC Version\n",
    "\n",
    "image_dir = '../../../RTC_Images/'\n",
    "\n",
    "image_name_list = np.array(pp_norm[9])\n",
    "\n",
    "# Train set\n",
    "P_train        = list()\n",
    "speed_train    = list()\n",
    "x_dir_train    = list()\n",
    "y_dir_train    = list()\n",
    "accel_train    = list()\n",
    "ed_train       = list()\n",
    "heating_train  = list()\n",
    "turning_train  = list()\n",
    "res_heat_train = list()\n",
    "image_name_train = list()\n",
    "label_train = list()\n",
    "\n",
    "for ii in tqdm(range(train_labels[0].shape[0])):\n",
    "    image_loc = train_labels[0][ii]\n",
    "    underscore = image_loc.find('_', 20)\n",
    "    if image_loc.find('Base') >= 0:\n",
    "        image_name = 'Base' + image_loc[underscore:]\n",
    "    else:\n",
    "        image_name = 'MPA' + image_loc[underscore:]\n",
    "        \n",
    "\n",
    "    image_name_indx = indx_dict[image_name]\n",
    "\n",
    "    P_train.append(pp_norm[0][image_name_indx])\n",
    "    speed_train.append(pp_norm[1][image_name_indx])\n",
    "    x_dir_train.append(pp_norm[2][image_name_indx])\n",
    "    y_dir_train.append(pp_norm[3][image_name_indx])\n",
    "    accel_train.append(pp_norm[4][image_name_indx])\n",
    "    ed_train.append(pp_norm[5][image_name_indx])\n",
    "    heating_train.append(pp_norm[6][image_name_indx])\n",
    "    turning_train.append(pp_norm[7][image_name_indx])\n",
    "    res_heat_train.append(pp_norm[8][image_name_indx])\n",
    "    image_name_train.append(image_name)\n",
    "    label_train.append(train_labels[1][ii])\n",
    "\n",
    "train_labels_pp = pd.DataFrame({'image_name': np.array(image_name_train), 'label': np.array(label_train),\n",
    "                               'P': np.array(P_train), 'speed': np.array(speed_train), 'x_dir': np.array(x_dir_train), \n",
    "                               'y_dir': np.array(y_dir_train), 'accel': np.array(accel_train), 'ed': np.array(ed_train), \n",
    "                               'heating': np.array(heating_train), 'turning': np.array(turning_train), \n",
    "                               'res_heat': np.array(res_heat_train)})\n",
    "\n",
    "train_labels_pp_limited = pd.DataFrame({'image_name': np.array(image_name_train), 'label': np.array(label_train),\n",
    "                               'P': np.array(P_train), 'speed': np.array(speed_train), 'accel': np.array(accel_train), \n",
    "                              'ed': np.array(ed_train), 'heating': np.array(heating_train), 'res_heat': np.array(res_heat_train)})\n",
    "                                     \n",
    "train_labels_pp.to_csv('neural_network_data/train_labels_pp_RTC6.csv',header=True, index=False)\n",
    "train_labels_pp_limited.to_csv('neural_network_data/train_labels_pp_limited_RTC6.csv',header=True, index=False)\n",
    "\n",
    "########################################################################################################################\n",
    "# dev set\n",
    "P_dev        = list()\n",
    "speed_dev    = list()\n",
    "x_dir_dev    = list()\n",
    "y_dir_dev    = list()\n",
    "accel_dev    = list()\n",
    "ed_dev       = list()\n",
    "heating_dev  = list()\n",
    "turning_dev  = list()\n",
    "res_heat_dev = list()\n",
    "image_name_dev = list()\n",
    "label_dev = list()\n",
    "\n",
    "for ii in tqdm(range(dev_labels[0].shape[0])):\n",
    "    image_loc = dev_labels[0][ii]\n",
    "    underscore = image_loc.find('_', 20)\n",
    "    if image_loc.find('Base') >= 0:\n",
    "        image_name = 'Base' + image_loc[underscore:]\n",
    "    else:\n",
    "        image_name = 'MPA' + image_loc[underscore:]\n",
    "    \n",
    "    image_name_indx = indx_dict[image_name]\n",
    "\n",
    "    P_dev.append(pp_norm[0][image_name_indx])\n",
    "    speed_dev.append(pp_norm[1][image_name_indx])\n",
    "    x_dir_dev.append(pp_norm[2][image_name_indx])\n",
    "    y_dir_dev.append(pp_norm[3][image_name_indx])\n",
    "    accel_dev.append(pp_norm[4][image_name_indx])\n",
    "    ed_dev.append(pp_norm[5][image_name_indx])\n",
    "    heating_dev.append(pp_norm[6][image_name_indx])\n",
    "    turning_dev.append(pp_norm[7][image_name_indx])\n",
    "    res_heat_dev.append(pp_norm[8][image_name_indx])\n",
    "    image_name_dev.append(image_name)\n",
    "    label_dev.append(dev_labels[1][ii])\n",
    "\n",
    "dev_labels_pp = pd.DataFrame({'image_name': np.array(image_name_dev), 'label': np.array(label_dev),\n",
    "                               'P': np.array(P_dev), 'speed': np.array(speed_dev), 'x_dir': np.array(x_dir_dev), \n",
    "                               'y_dir': np.array(y_dir_dev), 'accel': np.array(accel_dev), 'ed': np.array(ed_dev), \n",
    "                               'heating': np.array(heating_dev), 'turning': np.array(turning_dev), \n",
    "                               'res_heat': np.array(res_heat_dev)})\n",
    "\n",
    "dev_labels_pp_limited = pd.DataFrame({'image_name': np.array(image_name_dev), 'label': np.array(label_dev),\n",
    "                               'P': np.array(P_dev), 'speed': np.array(speed_dev), 'accel': np.array(accel_dev), \n",
    "                              'ed': np.array(ed_dev), 'heating': np.array(heating_dev), 'res_heat': np.array(res_heat_dev)})\n",
    "                                     \n",
    "dev_labels_pp.to_csv('neural_network_data/dev_labels_pp_RTC6.csv',header=True, index=False)\n",
    "dev_labels_pp_limited.to_csv('neural_network_data/dev_labels_pp_limited_RTC6.csv',header=True, index=False)\n",
    "\n",
    "########################################################################################################################\n",
    "# test set\n",
    "P_test        = list()\n",
    "speed_test    = list()\n",
    "x_dir_test    = list()\n",
    "y_dir_test    = list()\n",
    "accel_test    = list()\n",
    "ed_test       = list()\n",
    "heating_test  = list()\n",
    "turning_test  = list()\n",
    "res_heat_test = list()\n",
    "image_name_test = list()\n",
    "label_test = list()\n",
    "\n",
    "for ii in tqdm(range(test_labels[0].shape[0])):\n",
    "    image_loc = test_labels[0][ii]\n",
    "    underscore = image_loc.find('_', 20)\n",
    "    if image_loc.find('Base') >= 0:\n",
    "        image_name = 'Base' + image_loc[underscore:]\n",
    "    else:\n",
    "        image_name = 'MPA' + image_loc[underscore:]\n",
    "\n",
    "    image_name_indx = indx_dict[image_name]\n",
    "\n",
    "    P_test.append(pp_norm[0][image_name_indx])\n",
    "    speed_test.append(pp_norm[1][image_name_indx])\n",
    "    x_dir_test.append(pp_norm[2][image_name_indx])\n",
    "    y_dir_test.append(pp_norm[3][image_name_indx])\n",
    "    accel_test.append(pp_norm[4][image_name_indx])\n",
    "    ed_test.append(pp_norm[5][image_name_indx])\n",
    "    heating_test.append(pp_norm[6][image_name_indx])\n",
    "    turning_test.append(pp_norm[7][image_name_indx])\n",
    "    res_heat_test.append(pp_norm[8][image_name_indx])\n",
    "    image_name_test.append(image_name)\n",
    "    label_test.append(test_labels[1][ii])\n",
    "\n",
    "test_labels_pp = pd.DataFrame({'image_name': np.array(image_name_test), 'label': np.array(label_test),\n",
    "                               'P': np.array(P_test), 'speed': np.array(speed_test), 'x_dir': np.array(x_dir_test), \n",
    "                               'y_dir': np.array(y_dir_test), 'accel': np.array(accel_test), 'ed': np.array(ed_test), \n",
    "                               'heating': np.array(heating_test), 'turning': np.array(turning_test), \n",
    "                               'res_heat': np.array(res_heat_test)})\n",
    "\n",
    "test_labels_pp_limited = pd.DataFrame({'image_name': np.array(image_name_test), 'label': np.array(label_test),\n",
    "                               'P': np.array(P_test), 'speed': np.array(speed_test), 'accel': np.array(accel_test), \n",
    "                              'ed': np.array(ed_test), 'heating': np.array(heating_test), 'res_heat': np.array(res_heat_test)})\n",
    "                                     \n",
    "test_labels_pp.to_csv('neural_network_data/test_labels_pp_RTC6.csv',header=True, index=False)\n",
    "test_labels_pp_limited.to_csv('neural_network_data/test_labels_pp_limited_RTC6.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e087e35",
   "metadata": {},
   "source": [
    "### Provide oversampling of minority classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp = pd.read_csv('neural_network_data/train_labels_pp_limited_scan_3d_8.csv')\n",
    "# train_labels_pp = pd.read_csv('neural_network_data/train_labels_pp.csv')\n",
    "NUM_CLASSES = 8\n",
    "# NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current distribution of class data\n",
    "distribution = np.zeros((NUM_CLASSES,1))\n",
    "for i in range(NUM_CLASSES):\n",
    "    distribution[i] = np.sum(train_labels_pp['label']==i)\n",
    "plt.bar(range(NUM_CLASSES), distribution[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff60fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new balanced train set\n",
    "train_labels_pp_balanced = train_labels_pp.copy()\n",
    "\n",
    "max_class_size = np.max(distribution)\n",
    "for i in range(NUM_CLASSES):\n",
    "    num_additional_data = max_class_size - distribution[i,0]\n",
    "    \n",
    "    if num_additional_data > 0:\n",
    "        class_indices = np.where(train_labels_pp['label']==i)[0] # indices in the file of class i\n",
    "        repeat_indices = np.random.randint(0, len(class_indices), num_additional_data.astype('int')) # get indices to repeat (randomly)\n",
    "        new_data_indices = class_indices[repeat_indices] # now get the indices from the labels file\n",
    "        \n",
    "        train_labels_pp_balanced = pd.concat([train_labels_pp_balanced, train_labels_pp.iloc[new_data_indices]])\n",
    "        \n",
    "train_labels_pp_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by label\n",
    "train_labels_pp_balanced = train_labels_pp_balanced.sort_values(by=['label'])\n",
    "train_labels_pp_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now have even distribution of data\n",
    "new_distribution = np.zeros((NUM_CLASSES,1))\n",
    "for i in range(NUM_CLASSES):\n",
    "    new_distribution[i] = np.sum(train_labels_pp_balanced['label']==i)\n",
    "plt.bar(range(NUM_CLASSES), new_distribution[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4bf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "train_labels_pp_balanced.to_csv('neural_network_data/train_labels_pp_limited_scan_balanced_3d_8.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19431ba0",
   "metadata": {},
   "source": [
    "### Reduce data size for faster training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa83dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_NUM_IMAGES = 150000\n",
    "new_images_per_class = round(DESIRED_NUM_IMAGES / NUM_CLASSES)\n",
    "\n",
    "train_labels_pp_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = np.sum(train_labels_pp_balanced['label']==0)\n",
    "new_indx = np.random.choice(num_samples_per_class, size=new_images_per_class, replace=False)\n",
    "train_labels_pp_balanced_small = pd.DataFrame()\n",
    "for ii in range(NUM_CLASSES):\n",
    "    new_class_set = np.where(train_labels_pp_balanced['label']==ii)[0][new_indx]\n",
    "    train_labels_pp_balanced_small = pd.concat([train_labels_pp_balanced_small, train_labels_pp_balanced.iloc[new_class_set]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da00c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp_balanced_small.to_csv('neural_network_data/train_labels_pp_limited_scan_balanced_small_3d_8.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a928e",
   "metadata": {},
   "source": [
    "### Add X/Y Coordinates to Process Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bca31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_labels_pp = pd.read_csv('neural_network_data/train_labels_pp_RTC8.csv')\n",
    "train_labels_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp = train_labels_pp.sort_values(by=['image_name'])\n",
    "train_labels_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load x-y coordinates for all of these\n",
    "last_layer = 0\n",
    "image_names = np.array(train_labels_pp['image_name'])\n",
    "\n",
    "X_coord = np.zeros(len(train_labels_pp))\n",
    "Y_coord = np.zeros(len(train_labels_pp))\n",
    "\n",
    "for ii in tqdm(range(len(train_labels_pp))):\n",
    "    \n",
    "    current_image_name = image_names[ii]\n",
    "    current_layer = int(current_image_name[5:current_image_name.find('_')])\n",
    "    image_num = int(current_image_name[current_image_name.find('_')+1:current_image_name.find('.png')])\n",
    "    \n",
    "    # Load csv file if layer is different\n",
    "    if current_layer != last_layer:\n",
    "        last_layer = current_layer\n",
    "    \n",
    "        if current_layer < 10:\n",
    "            layer_text = 'layer000' + str(current_layer)\n",
    "        elif current_layer < 100:\n",
    "            layer_text = 'layer00' + str(current_layer)\n",
    "        else:\n",
    "            layer_text = 'layer0' + str(current_layer)\n",
    "              \n",
    "        data = pd.read_csv(DATA_DIR + 'T500_3D_Scan_Strategies_fused_' + layer_text + '.csv', header=None, \n",
    "                       names=['X', 'Y', 'P', 'T', 'deprecated'])\n",
    "        \n",
    "        # Collect the important info\n",
    "        X = np.array(data['X'])\n",
    "        Y = np.array(data['Y'])\n",
    "        T = np.array(data['T'])\n",
    "        indx = np.where(T!=0)\n",
    "        X = X[indx]\n",
    "        Y = Y[indx]\n",
    "\n",
    "    X_coord[ii] = X[image_num-1]\n",
    "    Y_coord[ii] = Y[image_num-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp['X'] = X_coord\n",
    "train_labels_pp['Y'] = Y_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afde529",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp = train_labels_pp.sort_values(by=['label'])\n",
    "train_labels_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as excel file\n",
    "train_labels_pp.to_excel('neural_network_data/train_labels_pp_x_y.xlsx',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206e127",
   "metadata": {},
   "source": [
    "### Visualize Process Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c684f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp = pd.read_excel('neural_network_data/train_labels_pp_x_y.xlsx')\n",
    "train_labels_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee830eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_labels_pp['X'])\n",
    "Y = np.array(train_labels_pp['Y'])\n",
    "P = np.array(train_labels_pp['P'])\n",
    "ed = np.array(train_labels_pp['ed'])\n",
    "labels = np.array(train_labels_pp['label'])\n",
    "res_heat = np.array(train_labels_pp['res_heat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,Y,c=res_heat, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2860cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(ed)), res_heat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62576313",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 'layer0029'\n",
    "pp_data = pd.read_excel('process_parameters/by_layer/' + layer_num + '.xlsx', header=None)\n",
    "P = np.array(pp_data[0])\n",
    "V = np.array(pp_data[1])\n",
    "ed = np.array(pp_data[5])\n",
    "res_heat = np.array(pp_data[8])\n",
    "# outlier_indx = np.where(ed>1)\n",
    "# ed[outlier_indx] = 0.6\n",
    "plt.scatter(range(len(ed)), ed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c172040",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_ed = (ed - np.mean(ed)) / np.std(ed)\n",
    "plt.scatter(range(len(ed)), normalize_ed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indx = np.where(ed>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648639b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[outlier_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "V[outlier_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fe65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ed = np.array(pp_data[5])\n",
    "new_ed[np.where(new_ed>1)] = 1\n",
    "pp_data[5] = new_ed\n",
    "np.max(new_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828fa17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d117465",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ed = np.array(pp_data[5])\n",
    "np.min(new_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6ace0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e4ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pp = pd.read_csv('neural_network_data/train_labels_pp_limited_RTC6.csv')\n",
    "labels = train_labels_pp['label']\n",
    "mean_pp = np.zeros((6,6))\n",
    "std_pp = np.zeros((6,6))\n",
    "for ii in range(6):\n",
    "    P_ii = train_labels_pp['P'][np.where(labels == ii)[0]]\n",
    "    speed_ii = train_labels_pp['speed'][np.where(labels == ii)[0]]\n",
    "    accel_ii = train_labels_pp['accel'][np.where(labels == ii)[0]]\n",
    "    ed_ii = train_labels_pp['ed'][np.where(labels == ii)[0]]\n",
    "    heating_ii = train_labels_pp['heating'][np.where(labels == ii)[0]]\n",
    "    res_heat_ii = train_labels_pp['res_heat'][np.where(labels == ii)[0]]\n",
    "    \n",
    "    mean_pp[ii,0] = np.mean(P_ii)\n",
    "    mean_pp[ii,1] = np.mean(speed_ii)\n",
    "    mean_pp[ii,2] = np.mean(accel_ii)\n",
    "    mean_pp[ii,3] = np.mean(ed_ii)\n",
    "    mean_pp[ii,4] = np.mean(heating_ii)\n",
    "    mean_pp[ii,5] = np.mean(res_heat_ii)\n",
    "    \n",
    "    std_pp[ii,0] = np.std(P_ii)\n",
    "    std_pp[ii,1] = np.std(speed_ii)\n",
    "    std_pp[ii,2] = np.std(accel_ii)\n",
    "    std_pp[ii,3] = np.std(ed_ii)\n",
    "    std_pp[ii,4] = np.std(heating_ii)\n",
    "    std_pp[ii,5] = np.std(res_heat_ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_labels = ['P', 'speed', 'accel', 'ed', 'heating', 'res_heat']\n",
    "for ii in range(6):\n",
    "    plt.errorbar(range(6), mean_pp[:,ii], std_pp[:,ii], linestyle='None', marker='^')\n",
    "    plt.title(pp_labels[ii])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(labels == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c6679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791d0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = image_name[506789]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_norm.to_excel('process_parameters/normalized_pp_no_outliers_with_scanstrategy.xlsx',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c86b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c5f0cee",
   "metadata": {},
   "source": [
    "### Remove the heating PP from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf05ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('neural_network_data/train_labels_pp_limited_scan_balanced_small_3d_8.csv')\n",
    "# new_train_data = train_data.drop(labels='heating', axis='columns')\n",
    "\n",
    "# dev_data = pd.read_csv('neural_network_data/dev_labels_pp_limited_scan_3d_8.csv')\n",
    "# new_dev_data = dev_data.drop(labels='heating', axis='columns')\n",
    "\n",
    "# test_data = pd.read_csv('neural_network_data/test_labels_pp_limited_scan_3d_8.csv')\n",
    "# new_test_data = test_data.drop(labels='heating', axis='columns')\n",
    "\n",
    "train_data = pd.read_csv('neural_network_data/train_labels_pp_limited_balanced_RTC6.csv')\n",
    "new_train_data = train_data.drop(labels='heating', axis='columns')\n",
    "\n",
    "dev_data = pd.read_csv('neural_network_data/dev_labels_pp_limited_RTC6.csv')\n",
    "new_dev_data = dev_data.drop(labels='heating', axis='columns')\n",
    "\n",
    "test_data = pd.read_csv('neural_network_data/test_labels_pp_limited_RTC6.csv')\n",
    "new_test_data = test_data.drop(labels='heating', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1caba5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_data.to_csv('neural_network_data/train_labels_pp_no_heating_scan_balanced_small_3d_8.csv', header=True, index=False)\n",
    "# new_dev_data.to_csv('neural_network_data/dev_labels_pp_no_heating_scan_3d_8.csv',header=True, index=False)\n",
    "# new_test_data.to_csv('neural_network_data/test_labels_pp_no_heating_scan_3d_8.csv',header=True, index=False)\n",
    "\n",
    "new_train_data.to_csv('neural_network_data/train_labels_pp_no_heating_balanced_RTC6.csv', header=True, index=False)\n",
    "new_dev_data.to_csv('neural_network_data/dev_labels_pp_no_heating_RTC6.csv',header=True, index=False)\n",
    "new_test_data.to_csv('neural_network_data/test_labels_pp_no_heating_RTC6.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7601a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meltpool_env",
   "language": "python",
   "name": "meltpool_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
